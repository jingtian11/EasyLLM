{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LoRALayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, rank):\n",
    "        super(LoRALayer, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.rank = rank\n",
    "\n",
    "        # Low-rank decomposition matrices\n",
    "        self.A = nn.Parameter(torch.randn(input_dim, rank))\n",
    "        self.B = nn.Parameter(torch.randn(rank, output_dim))\n",
    "\n",
    "        # Initialize the low-rank matrices\n",
    "        nn.init.kaiming_uniform_(self.A, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.B)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Compute the low-rank adaptation\n",
    "        delta_W = torch.matmul(self.A, self.B)\n",
    "        return torch.matmul(x, delta_W)\n",
    "\n",
    "class LoRA(nn.Module):\n",
    "    def __init__(self, base_model, lora_layers):\n",
    "        super(LoRA, self).__init__()\n",
    "        self.base_model = base_model\n",
    "        self.lora_layers = nn.ModuleList(lora_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass through the base model\n",
    "        base_output = self.base_model(x)\n",
    "\n",
    "        # Apply LoRA layers\n",
    "        for lora_layer in self.lora_layers:\n",
    "            base_output += lora_layer(x)\n",
    "\n",
    "        return base_output\n",
    "\n",
    "# Example usage\n",
    "input_dim = 256\n",
    "output_dim = 256\n",
    "rank = 4\n",
    "\n",
    "# Define a simple base model\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "base_model = SimpleModel(input_dim, output_dim)\n",
    "lora_layer = LoRALayer(input_dim, output_dim, rank)\n",
    "lora_model = LoRA(base_model, [lora_layer])\n",
    "\n",
    "# Create a random input tensor\n",
    "x = torch.rand((32, input_dim))\n",
    "\n",
    "# Apply the LoRA model\n",
    "output = lora_model(x)\n",
    "print(output.shape)  # Should output: torch.Size([32, output_dim])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
