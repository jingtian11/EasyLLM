{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class LlamaMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    LLaMA中的MLP层，使用SwiGLU激活函数\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, hidden_dim=None, multiple_of=256):\n",
    "        super().__init__()\n",
    "        # 如果未指定hidden_dim，则默认为输入维度的4倍\n",
    "        if hidden_dim is None:\n",
    "            hidden_dim = 4 * dim\n",
    "            \n",
    "        # LLaMA中的缩放策略，确保隐藏维度是multiple_of的倍数\n",
    "        # 这里使用2/3是SwiGLU的特殊需求\n",
    "        hidden_dim = int(2 * hidden_dim / 3)\n",
    "        # 向上取整到multiple_of的最近倍数\n",
    "        hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n",
    "        \n",
    "        # 三个线性投影，没有偏置项\n",
    "        self.gate_proj = nn.Linear(dim, hidden_dim, bias=False)  # W1\n",
    "        self.up_proj = nn.Linear(dim, hidden_dim, bias=False)    # W3\n",
    "        self.down_proj = nn.Linear(hidden_dim, dim, bias=False)  # W2\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # SwiGLU激活: (SiLU(W1x) ⊗ W3x)W2\n",
    "        # 其中SiLU(x) = x * sigmoid(x)，⊗表示元素级乘法\n",
    "        gate_output = F.silu(self.gate_proj(x))  # SiLU激活\n",
    "        up_output = self.up_proj(x)              # 上投影\n",
    "        intermediate = gate_output * up_output   # 元素级乘法\n",
    "        return self.down_proj(intermediate)      # 下投影"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
