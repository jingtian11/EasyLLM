{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    Root Mean Square Layer Normalization\n",
    "    \"\"\"\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 计算均方根(RMS)\n",
    "        rms = torch.sqrt(torch.mean(x ** 2, dim=-1, keepdim=True) + self.eps)\n",
    "        # 归一化\n",
    "        x_norm = x / rms\n",
    "        # 应用缩放参数\n",
    "        return self.weight * x_norm\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    Pre-normalization wrapper: 先归一化，再应用传入的模块\n",
    "    \"\"\"\n",
    "    def __init__(self, dim: int, fn, norm_type=\"rms\"):\n",
    "        super().__init__()\n",
    "        if norm_type == \"rms\":\n",
    "            self.norm = RMSNorm(dim)\n",
    "        else:\n",
    "            self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "    \n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "class SwiGLU(nn.Module):\n",
    "    \"\"\"\n",
    "    Swish-Gated Linear Unit activation function\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features: int, hidden_features: int, out_features: int, bias: bool = True):\n",
    "        super().__init__()\n",
    "        self.w1 = nn.Linear(in_features, hidden_features, bias=bias)\n",
    "        self.w2 = nn.Linear(in_features, hidden_features, bias=bias)\n",
    "        self.w3 = nn.Linear(hidden_features, out_features, bias=bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 实现SwiGLU激活函数\n",
    "        swish = self.w1(x) * torch.sigmoid(self.w1(x) * 1.0)  # β=1.0\n",
    "        gate = self.w2(x)\n",
    "        x = swish * gate  # 门控机制\n",
    "        x = self.w3(x)  # 投影到输出维度\n",
    "        return x\n",
    "\n",
    "class LLaMAFeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    LLaMA模型中的前馈网络模块\n",
    "    \"\"\"\n",
    "    def __init__(self, dim: int, hidden_dim: int, multiple_of: int = 256, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        # 确保hidden_dim是multiple_of的整数倍\n",
    "        hidden_dim = int(2 * hidden_dim / 3)\n",
    "        hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n",
    "        \n",
    "        self.swiglu = SwiGLU(dim, hidden_dim, dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.dropout(self.swiglu(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
