{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x110735f10>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===============================================================================\n",
    "# DPO（Direct Preference Optimization）算法实现\n",
    "# DPO通过人类偏好数据直接优化语言模型，使其生成更符合人类偏好的输出\n",
    "# 这里面使用了一个偏好prefer以及两个reject的格式\n",
    "# ===============================================================================\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import LlamaForCausalLM, LlamaConfig\n",
    "from copy import deepcopy\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(1000, 128)\n",
       "    (layers): ModuleList(\n",
       "      (0): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (k_proj): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (v_proj): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (o_proj): Linear(in_features=128, out_features=128, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=128, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=128, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=128, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((128,), eps=1e-06)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((128,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((128,), eps=1e-06)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=128, out_features=1000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载模型\n",
    "# 创建简化版的Llama模型作为策略模型（将被优化的模型）\n",
    "policy_model = LlamaForCausalLM(config=LlamaConfig(vocab_size=1000, num_hidden_layers=1, hidden_size=128))\n",
    "# 创建参考模型（通常是SFT模型，在训练过程中保持不变）\n",
    "reference_model = deepcopy(policy_model)  # 深度复制确保两个模型初始参数完全相同\n",
    "policy_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超参数\n",
    "beta = 0.1  # DPO的温度系数，控制策略模型与参考模型的偏离程度，值越小允许偏离越大\n",
    "\n",
    "# 准备训练数据\n",
    "# 在DPO中，我们需要提示(prompt)、优选回答(chosen/good)和拒绝回答(rejected/bad)\n",
    "prompt_ids = [1, 2, 3, 4, 5, 6]  # 输入提示的token IDs\n",
    "good_response_ids = [7, 8, 9, 10]  # 优质回答的token IDs\n",
    "# 多个低质量回答的示例，每个都是token IDs的列表\n",
    "bad_response_ids_list = [[1, 2, 0, 0], [4, 5, 6, 0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10],\n",
       "        [ 1,  2,  3,  4,  5,  6,  1,  2,  0,  0],\n",
       "        [ 1,  2,  3,  4,  5,  6,  4,  5,  6,  0]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 构建模型输入：将提示与回答拼接\n",
    "# 创建包含多个序列的批次：[提示+优质回答, 提示+低质回答1, 提示+低质回答2, ...]\n",
    "input_ids = torch.LongTensor(\n",
    "    [prompt_ids + good_response_ids, *[prompt_ids + bad_response_ids for bad_response_ids in bad_response_ids_list]]\n",
    ")\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-100, -100, -100, -100, -100,    7,    8,    9,   10],\n",
       "        [-100, -100, -100, -100, -100,    1,    2,    0,    0],\n",
       "        [-100, -100, -100, -100, -100,    4,    5,    6,    0]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 准备用于计算语言模型损失的标签\n",
    "# 在语言模型训练中，标签是输入向右移动一位（预测下一个token）\n",
    "# -100表示在计算损失时忽略该位置（这里忽略提示部分）\n",
    "labels = torch.LongTensor(\n",
    "    [\n",
    "        [-100] * len(prompt_ids) + good_response_ids,\n",
    "        *[[-100] * len(prompt_ids) + bad_response_ids for bad_response_ids in bad_response_ids_list]\n",
    "    ]\n",
    ")[:, 1:]  # 向右移动一位，因为我们预测的是下一个token\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0,  0,  0,  7,  8,  9, 10],\n",
       "        [ 0,  0,  0,  0,  0,  1,  2,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  4,  5,  6,  0]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建掩码，用于标识哪些位置参与损失计算（即回答部分）\n",
    "loss_mask = (labels != -100)\n",
    "# 将-100替换为0，因为在gather操作中-100是无效索引\n",
    "labels[labels == -100] = 0\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1456,  0.1039, -0.6784,  ..., -0.0864, -0.0790,  0.1382],\n",
       "         [ 0.2436,  0.1769, -0.0961,  ...,  0.0505,  0.1151, -0.1636],\n",
       "         [ 0.1643,  0.1157, -0.0293,  ...,  0.1549, -0.3632, -0.1554],\n",
       "         ...,\n",
       "         [-0.3279,  0.2739, -0.4922,  ..., -0.0714,  0.2901, -0.3667],\n",
       "         [-0.0849,  0.2404, -0.3772,  ..., -0.1673, -0.1023, -0.2427],\n",
       "         [-0.0503,  0.2694, -0.4878,  ...,  0.0252,  0.3393, -0.6586]],\n",
       "\n",
       "        [[ 0.1456,  0.1039, -0.6784,  ..., -0.0864, -0.0790,  0.1382],\n",
       "         [ 0.2436,  0.1769, -0.0961,  ...,  0.0505,  0.1151, -0.1636],\n",
       "         [ 0.1643,  0.1157, -0.0293,  ...,  0.1549, -0.3632, -0.1554],\n",
       "         ...,\n",
       "         [ 0.3901,  0.2500,  0.0026,  ...,  0.1113,  0.1034,  0.0109],\n",
       "         [ 0.2349,  0.2526,  0.0359,  ..., -0.0251, -0.1628, -0.4019],\n",
       "         [ 0.4680,  0.1479, -0.3143,  ...,  0.0656,  0.0351,  0.1007]],\n",
       "\n",
       "        [[ 0.1456,  0.1039, -0.6784,  ..., -0.0864, -0.0790,  0.1382],\n",
       "         [ 0.2436,  0.1769, -0.0961,  ...,  0.0505,  0.1151, -0.1636],\n",
       "         [ 0.1643,  0.1157, -0.0293,  ...,  0.1549, -0.3632, -0.1554],\n",
       "         ...,\n",
       "         [ 0.4758,  0.0963, -0.2063,  ...,  0.1648,  0.0475,  0.2014],\n",
       "         [ 0.2260,  0.0521, -0.2332,  ..., -0.2793,  0.0937,  0.0082],\n",
       "         [ 0.1226, -0.2356, -0.0755,  ..., -0.3023, -0.0541, -0.2869]]],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ===============================================================================\n",
    "# 计算策略模型（policy model）的对数概率\n",
    "# ===============================================================================\n",
    "# 前向传播，获取每个token位置的预测logits\n",
    "logits = policy_model(input_ids)[\"logits\"][:, :-1, :]  # 去掉最后一个位置，与label对齐\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-6.7857, -6.6947, -6.7697, -6.5169, -6.6811, -7.0314, -6.7662, -6.5559,\n",
       "         -6.6359],\n",
       "        [-6.7857, -6.6947, -6.7697, -6.5169, -6.6811, -7.1295, -6.9317, -6.6988,\n",
       "         -6.4697],\n",
       "        [-6.7857, -6.6947, -6.7697, -6.5169, -6.6811, -6.7405, -6.8247, -7.2198,\n",
       "         -6.8045]], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将logits转换为对数概率，并提取每个位置上正确token的对数概率\n",
    "per_token_logps = torch.gather(logits.log_softmax(-1), dim=2, index=labels.unsqueeze(2)).squeeze(2)\n",
    "per_token_logps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-60.4376, -60.6779, -61.0376], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 仅对回答部分（loss_mask=True的位置）求和，得到每个序列的总对数概率\n",
    "all_logps = (per_token_logps * loss_mask).sum(-1)\n",
    "all_logps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分离优质回答和低质量回答的对数概率\n",
    "policy_good_logps, policy_bad_logps = all_logps[:1], all_logps[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-60.4376], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_good_logps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-60.6779, -61.0376], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_bad_logps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits:\n",
      " tensor([[[ 0.1456,  0.1039, -0.6784,  ..., -0.0864, -0.0790,  0.1382],\n",
      "         [ 0.2436,  0.1769, -0.0961,  ...,  0.0505,  0.1151, -0.1636],\n",
      "         [ 0.1643,  0.1157, -0.0293,  ...,  0.1549, -0.3632, -0.1554],\n",
      "         ...,\n",
      "         [-0.3279,  0.2739, -0.4922,  ..., -0.0714,  0.2901, -0.3667],\n",
      "         [-0.0849,  0.2404, -0.3772,  ..., -0.1673, -0.1023, -0.2427],\n",
      "         [-0.0503,  0.2694, -0.4878,  ...,  0.0252,  0.3393, -0.6586]],\n",
      "\n",
      "        [[ 0.1456,  0.1039, -0.6784,  ..., -0.0864, -0.0790,  0.1382],\n",
      "         [ 0.2436,  0.1769, -0.0961,  ...,  0.0505,  0.1151, -0.1636],\n",
      "         [ 0.1643,  0.1157, -0.0293,  ...,  0.1549, -0.3632, -0.1554],\n",
      "         ...,\n",
      "         [ 0.3901,  0.2500,  0.0026,  ...,  0.1113,  0.1034,  0.0109],\n",
      "         [ 0.2349,  0.2526,  0.0359,  ..., -0.0251, -0.1628, -0.4019],\n",
      "         [ 0.4680,  0.1479, -0.3143,  ...,  0.0656,  0.0351,  0.1007]],\n",
      "\n",
      "        [[ 0.1456,  0.1039, -0.6784,  ..., -0.0864, -0.0790,  0.1382],\n",
      "         [ 0.2436,  0.1769, -0.0961,  ...,  0.0505,  0.1151, -0.1636],\n",
      "         [ 0.1643,  0.1157, -0.0293,  ...,  0.1549, -0.3632, -0.1554],\n",
      "         ...,\n",
      "         [ 0.4758,  0.0963, -0.2063,  ...,  0.1648,  0.0475,  0.2014],\n",
      "         [ 0.2260,  0.0521, -0.2332,  ..., -0.2793,  0.0937,  0.0082],\n",
      "         [ 0.1226, -0.2356, -0.0755,  ..., -0.3023, -0.0541, -0.2869]]])\n",
      "per_token_logps:\n",
      " tensor([[-6.7857, -6.6947, -6.7697, -6.5169, -6.6811, -7.0314, -6.7662, -6.5559,\n",
      "         -6.6359],\n",
      "        [-6.7857, -6.6947, -6.7697, -6.5169, -6.6811, -7.1295, -6.9317, -6.6988,\n",
      "         -6.4697],\n",
      "        [-6.7857, -6.6947, -6.7697, -6.5169, -6.6811, -6.7405, -6.8247, -7.2198,\n",
      "         -6.8045]])\n",
      "all_logps\n",
      " tensor([-60.4376, -60.6779, -61.0376])\n",
      "reference_good_logps:\n",
      " tensor([-60.4376])\n",
      "reference_bad_logps\n",
      " tensor([-60.6779, -61.0376])\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================================\n",
    "# 计算参考模型（reference model）的对数概率\n",
    "# ===============================================================================\n",
    "with torch.no_grad():  # 不计算梯度，因为参考模型不需要更新\n",
    "    # 重复与策略模型相同的步骤\n",
    "    logits = reference_model(input_ids)[\"logits\"][:, :-1, :]\n",
    "    print(\"logits:\\n\",logits)\n",
    "    per_token_logps = torch.gather(logits.log_softmax(-1), dim=2, index=labels.unsqueeze(2)).squeeze(2)\n",
    "    print(\"per_token_logps:\\n\",per_token_logps)\n",
    "    all_logps = (per_token_logps * loss_mask).sum(-1)\n",
    "    print(\"all_logps\\n\",all_logps)\n",
    "    reference_good_logps, reference_bad_logps = all_logps[:1], all_logps[1:]\n",
    "    print(\"reference_good_logps:\\n\",reference_good_logps)\n",
    "    print(\"reference_bad_logps\\n\",reference_bad_logps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6931, grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# ===============================================================================\n",
    "# 计算DPO损失\n",
    "# DPO的核心思想：增大策略模型对优质回答的概率，同时减小对低质量回答的概率\n",
    "# ===============================================================================\n",
    "# 计算DPO的logits：(策略模型相对于参考模型对好回答的提升) - (对坏回答的提升)\n",
    "logits = (policy_good_logps - reference_good_logps) - (policy_bad_logps - reference_bad_logps)\n",
    "# 应用logsigmoid函数并乘以beta控制优化强度，取负值（因为要最小化损失）\n",
    "loss = -F.logsigmoid(beta * logits).mean()  # 对所有样本取平均\n",
    "\n",
    "# 输出损失值\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
